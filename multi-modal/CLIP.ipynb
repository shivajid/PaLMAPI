{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73652c6-9746-4903-828c-52659bce0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection, CLIPVisionModelWithProjection,  AutoProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f09f92-2644-471a-bc7d-9cb22d2cd4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPTextModelWithProjection: ['vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644cb58-844a-495a-b387-40e84eb3667b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ac501e-4fe5-416f-bb3a-bce40050b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"a pair of Kittens sleeping\",\"a pair of Kittens sleeping\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad604f5-3fa1-40f8-9193-9d693622c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71939566-9059-4767-8e29-598c46d0c663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModelOutput(text_embeds=tensor([[ 0.0786, -0.1164, -0.4900,  ..., -0.1514, -0.4523, -0.2023],\n",
       "        [ 0.0786, -0.1164, -0.4900,  ..., -0.1514, -0.4523, -0.2023],\n",
       "        [ 0.0932,  0.2764, -0.4137,  ..., -0.5852, -0.2590,  0.1193]],\n",
       "       grad_fn=<MmBackward0>), last_hidden_state=tensor([[[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 0.2446, -0.0337,  0.0675,  ..., -0.0108, -0.1678, -2.1158],\n",
       "         ...,\n",
       "         [ 0.3420, -0.5756,  1.2779,  ..., -0.0484,  0.5701, -1.7490],\n",
       "         [ 0.5918, -0.3644,  0.1242,  ..., -0.2798,  1.1070, -1.0061],\n",
       "         [-0.3823,  0.4144,  1.6134,  ...,  0.0771,  0.5741, -1.9464]],\n",
       "\n",
       "        [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 0.2446, -0.0337,  0.0675,  ..., -0.0108, -0.1678, -2.1158],\n",
       "         ...,\n",
       "         [ 0.3420, -0.5756,  1.2779,  ..., -0.0484,  0.5701, -1.7490],\n",
       "         [ 0.5918, -0.3644,  0.1242,  ..., -0.2798,  1.1070, -1.0061],\n",
       "         [-0.3823,  0.4144,  1.6134,  ...,  0.0771,  0.5741, -1.9464]],\n",
       "\n",
       "        [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "         ...,\n",
       "         [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "         [-0.1433, -0.5163,  1.7099,  ..., -0.0795,  0.3609, -1.2437],\n",
       "         [ 0.0426,  0.0189,  1.2740,  ..., -0.4217, -0.4393, -1.3016]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf23361a-8860-4a9d-9687-a24c91c51ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeds = outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "973cf998-27ce-46e7-83db-3a63f6cf4f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 np.array(text_embeds[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>].tolist())                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'np'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 np.array(text_embeds[\u001b[94m1\u001b[0m].tolist())                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'np'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(text_embeds[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5c43b1-bc72-42a1-8c4a-3d92022ab6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_projection.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.final_layer_norm.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da62e3c-72ef-4b80-bb12-157457638acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6686067b-4e0e-43f0-90b9-817bd1f49f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb9bf8e-608a-469b-9386-f3b8bdeb0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb6222f-4fb7-4b6f-885d-99d499b66e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds = outputs.image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f133d5e-2fcc-40da-8e75-7d3812670f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdf96155-e1a0-423d-9745-f3b71038b5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_embeds.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e80f74b-8935-44cd-b639-ad99b6441084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72dcaf14-df13-41fa-a376-96ada51b20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = np.array(image_embeds.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8581336-751b-439c-afe7-bf1e9adf9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_arr = np.array(text_embeds[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f928f494-7dec-4c7f-93b0-db5f778cb75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.27812504571261"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(img_arr, text_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c425c65-591d-4e94-9ec3-c9e2fd9c0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb4f02f0-6ff1-42d8-af55-b6ec29699a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = np.dot(img_arr, text_arr)/(norm(img_arr)*norm(text_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0355e64-c7ed-4f47-a117-0eba22321fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2940507353334896"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d71ac492-f066-4a99-93b2-c994f5b08bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 similarity_score = torch.nn.functional.cosine_similarity(text_embeds[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], text_embeds[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>])     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>Dimension out of range <span style=\"font-weight: bold\">(</span>expected to be in range of <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, but got <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 similarity_score = torch.nn.functional.cosine_similarity(text_embeds[\u001b[94m1\u001b[0m], text_embeds[\u001b[94m2\u001b[0m])     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mIndexError: \u001b[0mDimension out of range \u001b[1m(\u001b[0mexpected to be in range of \u001b[1m[\u001b[0m\u001b[1;36m-1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, but got \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity_score = torch.nn.functional.cosine_similarity(text_embeds[1], text_embeds[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd662569-1cf9-4d9d-a78b-ce1da8eafbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 similarity_score                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'similarity_score'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 similarity_score                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'similarity_score'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1bc4de-a2c6-41fb-8f11-e1f4d897fd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.8639e-02, -1.1640e-01, -4.8996e-01, -1.8113e-02, -1.2069e-01,\n",
       "         2.7287e-01, -3.5920e-01, -5.1525e-01, -3.8022e-01, -1.5557e-01,\n",
       "         1.0042e-01, -2.6965e-01, -2.7772e-01, -6.5565e-01, -5.7275e-01,\n",
       "         2.4773e-01, -2.5985e-01, -1.3073e-01,  1.5946e-01,  6.2904e-01,\n",
       "        -1.1369e-01, -7.7783e-02,  1.0770e-01, -1.3284e-01,  3.0105e-02,\n",
       "         1.3388e-01,  1.5950e-01,  7.7331e-01, -1.3675e-01, -6.5382e-02,\n",
       "         8.0658e-02, -8.6160e-02,  1.6838e-01,  3.9098e-01, -9.9478e-01,\n",
       "         5.9946e-03,  3.2895e-01,  3.1184e-01,  1.0530e-01,  9.1251e-02,\n",
       "         2.5212e-01, -1.1359e-01, -6.3757e-02, -5.5682e-02,  3.2144e-01,\n",
       "         5.0882e-02, -4.5993e-02, -1.5483e-01, -1.1238e-01, -1.1533e-01,\n",
       "         4.2757e-01, -2.0133e-02,  3.1974e-01, -1.8372e-01, -1.1654e-01,\n",
       "         7.1383e-03, -6.8467e-01,  2.2001e-01, -1.3863e-02, -2.9394e-01,\n",
       "        -1.1392e-02, -1.2948e-01, -1.7851e-01, -3.4290e-01,  2.3019e-01,\n",
       "        -5.4519e-02, -5.7900e-03,  2.7436e-01, -3.8254e-01, -1.0390e-01,\n",
       "         1.4923e-01,  1.5223e-01,  2.1659e-01,  2.0700e-01,  1.2176e-01,\n",
       "        -1.5909e-02,  1.4288e-01,  1.8863e-01, -4.9379e-02, -4.7165e-02,\n",
       "        -3.9495e-01, -2.0145e-01, -1.9661e-01,  3.1159e-02, -1.9905e-01,\n",
       "         3.3064e-01,  3.4180e-01,  1.0269e-01, -6.1286e-02,  8.1519e-02,\n",
       "         2.7802e-01,  1.3153e-01, -6.7900e-01,  8.4607e-01,  7.4056e-02,\n",
       "        -5.4121e-02,  1.4109e-01,  1.6145e-01, -3.2039e-01, -1.9344e-01,\n",
       "         1.7370e-01, -1.9887e-02, -5.8875e-02,  6.0986e-01,  5.2774e-01,\n",
       "         9.0562e-02, -1.2474e-01, -9.2493e-02, -2.9285e-01, -7.8976e-02,\n",
       "        -3.6478e-02, -3.6590e-01, -2.7838e-01,  2.9695e-01, -8.7417e-02,\n",
       "         3.6254e-01, -3.6170e-01,  8.0422e-02,  9.8226e-02,  4.6441e-02,\n",
       "        -9.9505e-02, -9.7995e-01, -4.0082e-02, -4.2399e-02,  1.3113e-01,\n",
       "        -3.6738e-01, -3.6112e-01, -1.9465e-01,  2.0255e-02,  4.7976e-01,\n",
       "        -2.4460e-01,  4.1135e-01, -1.7493e-01,  3.9123e+00, -2.5274e-01,\n",
       "        -2.2399e-01,  1.1714e-02,  1.2984e-01, -1.5114e-01, -1.6375e-01,\n",
       "         1.0466e-01, -3.1736e-02,  2.8925e-01, -4.8433e-01, -5.8490e-01,\n",
       "        -3.0169e-01,  2.0469e-02, -1.0045e+00, -3.6275e-02,  7.9029e-03,\n",
       "         3.4726e-01,  9.1229e-02, -1.4305e-01,  3.0297e-01, -1.4875e-01,\n",
       "        -3.2959e-01,  1.7060e-01, -3.4798e-01,  3.2879e-01, -4.5805e-01,\n",
       "         2.0576e-01,  3.5484e-01, -1.1083e-01, -1.5077e-01,  2.5363e-01,\n",
       "         5.1394e-02, -1.6551e-01, -6.5918e-02, -1.0390e-01, -3.2610e-01,\n",
       "         6.4521e-02,  1.2441e-01, -1.0217e-01, -1.2310e-01, -6.7455e-01,\n",
       "         8.6197e-01,  1.1456e-02, -1.0220e-01, -2.2380e-01,  1.5219e-01,\n",
       "        -5.0563e-02,  3.1626e-01, -6.3929e-01, -1.3428e-01, -3.0031e-01,\n",
       "         7.0597e-02, -4.4170e-02, -1.9389e-01, -6.9512e-02,  7.3928e-02,\n",
       "         1.4152e-02, -2.2056e-01, -1.9768e-01,  1.3245e-01, -3.9730e-01,\n",
       "        -2.0239e-01, -2.7254e-01, -2.5182e-01, -6.5153e-01, -2.1445e-01,\n",
       "         5.8650e-02,  1.9443e-01, -2.0450e-01,  3.6294e-02,  2.2946e-01,\n",
       "         1.1796e-01,  9.5445e-02, -3.2368e-01,  1.5458e-01,  2.1681e-02,\n",
       "         5.4425e-02,  8.3117e-01,  2.1382e-01, -2.1847e-01,  9.8707e-02,\n",
       "        -3.8164e-01,  2.9132e-01, -4.7162e-01,  6.4151e-01, -9.2172e-02,\n",
       "        -1.3325e-01, -2.8667e-01, -1.2314e-01,  2.7367e-01,  3.6713e-01,\n",
       "        -2.8161e-01,  4.7260e-01, -9.6719e-04,  1.5904e-01, -1.9201e-01,\n",
       "         3.7281e-02,  5.8286e-02,  3.4785e-04,  4.8715e-01,  3.3201e-01,\n",
       "         4.9203e-01,  5.4705e-02,  3.4812e-02,  1.8412e-01, -4.8792e-02,\n",
       "         2.0250e-01, -1.1345e-01, -1.5738e-01,  3.2691e-01,  4.7120e-02,\n",
       "        -1.6630e-01, -7.6696e-02,  6.2112e-02, -5.9503e-02, -1.6426e-01,\n",
       "        -3.0393e-02, -1.6267e-01,  4.8787e-02, -3.3716e-01, -2.8521e-01,\n",
       "         1.6205e-01, -2.8677e-01,  4.1204e-02, -2.1139e-02, -6.5575e-03,\n",
       "         3.5811e-02, -4.4664e-02,  4.5386e-02,  1.3589e-01, -6.7744e-02,\n",
       "        -1.1881e-01, -2.3573e-01, -1.9611e-02, -1.6151e-01,  5.7406e-02,\n",
       "        -1.9571e-01,  1.3793e-01, -2.3737e-01, -3.6969e-01, -3.0365e-01,\n",
       "        -2.7794e-01,  1.7727e-01,  6.1860e-02,  5.0036e-02,  9.4713e-02,\n",
       "         4.2829e-01,  4.6322e-01,  1.0036e-01,  1.8790e-01,  3.4385e-01,\n",
       "        -2.1395e-01,  1.3380e-01,  1.3386e-01,  3.5816e-01,  2.0449e-01,\n",
       "         1.3987e-01,  3.1246e-01,  1.8033e-01,  2.1898e-01,  8.2279e-02,\n",
       "         1.4961e-01,  1.3972e-01, -2.2097e-01, -6.1137e-02,  2.6215e-01,\n",
       "         2.9294e-02,  4.0837e-01, -4.4886e-01,  1.6723e-01, -2.3854e-01,\n",
       "        -1.1041e-01,  1.1878e-01,  5.5960e-01, -4.9024e-01,  3.7327e-01,\n",
       "         7.2329e-02,  2.8605e-02,  3.9071e+00,  1.2882e-01,  1.0885e-01,\n",
       "        -1.2293e-01,  4.5684e-02,  4.3437e-01,  2.1982e-01,  2.3859e-01,\n",
       "        -1.4860e-01,  5.0539e-01,  2.9435e-01, -3.0818e-01, -4.8110e-02,\n",
       "        -3.4902e-01,  5.1615e-01,  1.0629e-01,  1.3831e-01, -3.9584e-01,\n",
       "        -1.4069e-01,  4.5355e-01,  5.0505e-02, -1.9634e-01,  4.8144e-01,\n",
       "        -1.3050e-01,  3.7776e-04, -2.6970e-03, -1.5709e-03,  1.0744e-01,\n",
       "        -3.1001e-01,  2.0389e-01,  2.4818e-01, -2.4606e-01,  1.8078e-01,\n",
       "        -5.5159e-01, -8.5539e-02,  1.1463e-01, -2.9159e-01, -1.3785e-01,\n",
       "        -1.3440e-02, -1.5337e-02,  4.0568e-01, -9.3947e-02, -3.3123e-01,\n",
       "        -7.3687e-02, -1.1245e-01, -1.3932e-01, -2.3222e-01,  9.1298e-02,\n",
       "        -1.2919e-01, -4.3449e-01,  2.0592e-01,  8.7884e-02, -4.9964e-02,\n",
       "         2.5657e-02,  1.1670e-01,  1.2930e-01,  8.0490e-02, -1.2935e-02,\n",
       "         2.8101e-02,  2.6801e-01, -2.3806e-01, -1.0542e-01,  2.3181e-01,\n",
       "        -5.3582e-02,  3.9889e-01, -1.9680e-01, -2.4670e-01, -1.9796e-01,\n",
       "        -2.8461e-01, -8.8380e-02, -5.7857e-01, -1.5468e-01, -1.6789e-01,\n",
       "         1.6978e-01, -8.5250e-02, -2.1229e-04, -2.5884e-03, -8.1103e-02,\n",
       "         1.5888e-01, -6.1861e-02, -2.5628e-01,  2.9803e-01,  2.9326e-01,\n",
       "         3.4389e-01,  2.3238e-01, -7.8950e-02, -1.9456e-01,  5.6531e-01,\n",
       "         2.2911e-01, -3.0744e-02,  4.8569e-01,  3.9848e-01, -7.8626e-02,\n",
       "         1.0072e-01, -1.3248e-01,  5.6197e-02,  5.5992e-01,  4.7904e-01,\n",
       "         1.5272e-01,  5.4629e-01,  3.8063e-02,  2.7736e-01, -1.7009e-02,\n",
       "        -1.5103e-01, -2.9841e-01, -7.6127e-02, -3.7133e-01, -1.7269e-01,\n",
       "        -1.6393e-01,  1.4576e-02, -2.1897e-01, -1.5162e-01,  4.6144e-01,\n",
       "         6.3414e-02,  2.1786e-01, -6.2486e-02,  1.5768e-01, -1.7132e-01,\n",
       "         6.9611e-02,  2.2535e-01, -2.4224e-01,  6.3807e-01, -1.7879e-01,\n",
       "        -3.9645e-01, -2.9031e-01,  4.1517e-01,  8.9769e-02,  5.1926e-02,\n",
       "        -8.3118e-02,  3.1077e-02,  1.7311e-01, -3.5436e-02, -1.8750e-01,\n",
       "         3.7557e-01,  4.3456e-02, -1.0285e-01,  2.6016e-01, -2.2625e-01,\n",
       "        -1.4785e-01,  1.3544e-01, -3.2698e-02, -2.2330e-01,  1.8026e-01,\n",
       "        -1.3822e-01, -1.0846e-01, -1.3368e-03, -1.4603e-01,  9.1824e-02,\n",
       "         7.3765e-02,  1.6160e-01, -2.9284e-01,  2.0428e-02,  2.8277e-01,\n",
       "        -1.2526e-01,  2.2638e-01, -2.0220e-01, -1.7119e-02, -4.0326e-01,\n",
       "        -1.2533e-01, -1.8393e-01, -1.7486e-01, -6.3438e-03, -2.9747e-01,\n",
       "        -4.7903e-01, -1.7109e-01,  4.6844e-02, -2.3380e-02,  3.3366e-01,\n",
       "         1.5573e-02,  1.5065e-02,  2.8032e-01,  1.4083e-01,  5.4645e-02,\n",
       "         1.3349e-03, -1.0045e-01, -1.6128e-01,  9.7964e-02,  1.5469e-01,\n",
       "         4.6959e-02, -5.3657e-01, -1.4910e-01,  1.4289e+00,  1.9095e-01,\n",
       "         8.0929e-02,  1.1707e-01,  1.0344e-01,  1.5212e-02,  2.9087e-01,\n",
       "         3.8773e-01,  1.7043e-01,  5.1497e-01,  1.3576e-02, -4.1511e-02,\n",
       "        -3.0470e-02, -1.7840e-01,  1.0928e-01,  4.4367e-01, -1.5144e-01,\n",
       "        -4.5230e-01, -2.0226e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50014ab-2ac9-4a1c-93c8-e0705fa24d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
