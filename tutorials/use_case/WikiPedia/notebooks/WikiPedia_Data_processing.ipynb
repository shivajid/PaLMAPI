{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia Data Extractor\n",
        "\n",
        "In this notebook we will crawl  Wikipedia extract pages about Beijing Olympics 2022, which we will later use with Vertex PaLM API.\n",
        "\n"
      ],
      "metadata": {
        "id": "vH_Ruw87l1ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Authenticate with your google cloud account\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "PQ1vk3S8qg4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all the needed packages\n",
        "\n",
        "#Midiawiki client\n",
        "!pip install mwclient\n",
        "\n",
        "#MidiaWiki parser\n",
        "!pip install mwparserfromhell\n"
      ],
      "metadata": {
        "id": "RjMTWU3SmvuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7acd63-370a-4054-f6ef-a5cd7d106093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mwclient\n",
            "  Downloading mwclient-0.10.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from mwclient) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mwclient) (1.16.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->mwclient) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->mwclient) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (3.4)\n",
            "Installing collected packages: mwclient\n",
            "Successfully installed mwclient-0.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mwparserfromhell\n",
            "  Downloading mwparserfromhell-0.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mwparserfromhell\n",
            "Successfully installed mwparserfromhell-0.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the needed libraries for this module\n",
        "\n",
        "# Import packages\n",
        "import re\n",
        "import pandas as pd\n",
        "import mwclient\n",
        "import mwparserfromhell\n"
      ],
      "metadata": {
        "id": "qGt_5JVIrKcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get Wikipedia pages about the 2022 Winter Olympics\n",
        "\n",
        "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"#@param\n",
        "WIKI_SITE = \"en.wikipedia.org\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjHapW02rQaR",
        "outputId": "12fb7d43-89b9-4698-87c4-074aa1e545c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def titles_from_category(\n",
        "    category: mwclient.listing.Category, max_depth: int\n",
        ") -> set[str]:\n",
        "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
        "    titles = set()\n",
        "    for cm in category.members():\n",
        "        if type(cm) == mwclient.page.Page:\n",
        "            # ^type() used instead of isinstance() to catch match w/ no inheritance\n",
        "            titles.add(cm.name)\n",
        "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
        "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
        "            titles.update(deeper_titles)\n",
        "    return titles\n",
        "\n",
        "\n",
        "site = mwclient.Site(WIKI_SITE)\n",
        "category_page = site.pages[CATEGORY_TITLE]\n",
        "titles = titles_from_category(category_page, max_depth=1)\n",
        "# ^note: max_depth=1 means we go one level deep in the category tree\n",
        "print(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FuPa4NbrU9x",
        "outputId": "b03a05a8-862f-468e-f777-0c9f6c33efec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 732 article titles in Category:2022 Winter Olympics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our reference documents, we need to prepare them for search.\n",
        "\n",
        "Because PaLM TextEmbedding can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
        "\n",
        "For this specific example on Wikipedia articles, we'll:\n",
        "\n",
        "Discard less relevant-looking sections like External Links and Footnotes\n",
        "Clean up the text by removing reference tags (e.g., ), whitespace, and super short sections\n",
        "Split each article into sections\n",
        "Prepend titles and subtitles to each section's text, to help PaLM understand the context\n",
        "If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
      ],
      "metadata": {
        "id": "jA0nHCIGrjgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SECTIONS_TO_IGNORE = [\n",
        "    \"See also\",\n",
        "    \"References\",\n",
        "    \"External links\",\n",
        "    \"Further reading\",\n",
        "    \"Footnotes\",\n",
        "    \"Bibliography\",\n",
        "    \"Sources\",\n",
        "    \"Citations\",\n",
        "    \"Literature\",\n",
        "    \"Footnotes\",\n",
        "    \"Notes and references\",\n",
        "    \"Photo gallery\",\n",
        "    \"Works cited\",\n",
        "    \"Photos\",\n",
        "    \"Gallery\",\n",
        "    \"Notes\",\n",
        "    \"References and sources\",\n",
        "    \"References and notes\",\n",
        "]\n",
        "\n",
        "\n",
        "def all_subsections_from_section(\n",
        "    section: mwparserfromhell.wikicode.Wikicode,\n",
        "    parent_titles: list[str],\n",
        "    sections_to_ignore: set[str],\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"\n",
        "    From a Wikipedia section, return a flattened list of all nested subsections.\n",
        "    Each subsection is a tuple, where:\n",
        "        - the first element is a list of parent subtitles, starting with the page title\n",
        "        - the second element is the text of the subsection (but not any children)\n",
        "    \"\"\"\n",
        "    headings = [str(h) for h in section.filter_headings()]\n",
        "    title = headings[0]\n",
        "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
        "        # ^wiki headings are wrapped like \"== Heading ==\"\n",
        "        return []\n",
        "    titles = parent_titles + [title]\n",
        "    full_text = str(section)\n",
        "    section_text = full_text.split(title)[1]\n",
        "    if len(headings) == 1:\n",
        "        return [(titles, section_text)]\n",
        "    else:\n",
        "        first_subtitle = headings[1]\n",
        "        section_text = section_text.split(first_subtitle)[0]\n",
        "        results = [(titles, section_text)]\n",
        "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
        "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
        "        return results\n",
        "\n",
        "\n",
        "def all_subsections_from_title(\n",
        "    title: str,\n",
        "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
        "    site_name: str = WIKI_SITE,\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
        "    Each subsection is a tuple, where:\n",
        "        - the first element is a list of parent subtitles, starting with the page title\n",
        "        - the second element is the text of the subsection (but not any children)\n",
        "    \"\"\"\n",
        "    site = mwclient.Site(site_name)\n",
        "    page = site.pages[title]\n",
        "    text = page.text()\n",
        "    parsed_text = mwparserfromhell.parse(text)\n",
        "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
        "    if headings:\n",
        "        summary_text = str(parsed_text).split(headings[0])[0]\n",
        "    else:\n",
        "        summary_text = str(parsed_text)\n",
        "    results = [([title], summary_text)]\n",
        "    for subsection in parsed_text.get_sections(levels=[2]):\n",
        "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
        "    return results\n",
        "\n",
        "# split pages into sections\n",
        "# may take ~1 minute per 100 articles\n",
        "wikipedia_sections = []\n",
        "for title in titles:\n",
        "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
        "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiHpUsb-rcMw",
        "outputId": "d9fc4c92-254a-46e0-a3d6-cce48854a9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5718 sections in 732 pages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for English Language. Change this for other languagse\n",
        "tokens_to_char = 4 #@param"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3HRceH9r8W_",
        "outputId": "23817488-5cc6-4278-f878-e5dab12a7cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MAX_INPUT_TOKEN = 4095 # One less than Max for PaLM API of 4096\n",
        "\n",
        "\n",
        "def num_tokens(text: str) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    strlength =  len(text)\n",
        "    tokens = int(strlength/4)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # no delimiter found\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # no need to search for halfway point\n",
        "    else:\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        best_diff = halfway\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "def truncated_string(\n",
        "    string: str,\n",
        "    max_tokens: int,\n",
        "    print_warning: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
        "    truncated_string = string[:MAX_INPUT_TOKEN]\n",
        "    if print_warning and len(string) > MAX_INPUT_TOKEN:\n",
        "        print(f\"Warning: Truncated string from {len(string)} tokens to {MAX_INPUT_TOKEN} tokens.\")\n",
        "    return truncated_string\n",
        "\n",
        "\n",
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str],\n",
        "    max_tokens: int = 4095,\n",
        "    max_recursion: int = 5,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
        "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # if length is fine, return string\n",
        "    if num_tokens_in_string <= MAX_INPUT_TOKEN:\n",
        "        return [string]\n",
        "    # if recursion hasn't found a split after X iterations, just truncate\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, max_tokens=MAX_INPUT_TOKEN)]\n",
        "    # otherwise, split in half and recurse\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # if either half is empty, retry with a more fine-grained delimiter\n",
        "                continue\n",
        "            else:\n",
        "                # recurse on each half\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        max_recursion=max_recursion - 1,\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # otherwise no split was found, so just truncate (should be very rare)\n",
        "    return [truncated_string(string, max_tokens=max_tokens)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulrSR9-0rrPW",
        "outputId": "f79868c4-ad0b-4aed-c1b0-26ac9a8e8e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split sections into chunks\n",
        "MAX_TOKENS = 1600\n",
        "wikipedia_strings = []\n",
        "for section in wikipedia_sections:\n",
        "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-8-a-SMsJnm",
        "outputId": "a7174a71-4a27-48a5-b963-01cbf9164b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Truncated string from 16785 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 18745 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 19558 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 24852 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 16401 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 35395 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 26974 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 21462 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 36966 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 29982 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 19095 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 19262 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 16651 tokens to 4095 tokens.\n",
            "Warning: Truncated string from 22333 tokens to 4095 tokens.\n",
            "5718 Wikipedia sections split into 5780 strings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Validate to see the max Size of the chuncks\n",
        "len_str = []\n",
        "for wik in wikipedia_strings:\n",
        " len_str.append(len(wik))\n",
        "\n",
        "len_str.sort(reverse=True)\n",
        "len_str[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5y2ErL0sPuF",
        "outputId": "f0da9afa-2cdd-47d3-b37e-f5ebf3f3ec8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16307, 16261, 16044, 15979, 15800]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the Chunked documents in a Dataframe to persist to storage\n",
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df[\"text\"] = wikipedia_strings\n",
        "df.to_csv(\"wikipedia_strings.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhPRyROCsYa1",
        "outputId": "ac987e1a-18a1-4975-f8c4-992d57a4084b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PROJECT_ID = \"demogct2022\" #@param\n",
        "BUCKET_TO_SAVE= f\"wikipedia_strings_{PROJECT_ID}\"#@param\n",
        "REGION =\"us-central1\"#@param"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYfmdRXksp3N",
        "outputId": "d992eb1d-f170-4208-abf5-905b1ba3d6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7EoB4tq6zcm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import storage\n",
        "\n",
        "# Create a storage client\n",
        "client = storage.Client(project=PROJECT_ID)\n",
        "bucket = client.bucket(BUCKET_TO_SAVE)\n",
        "bucket.location = REGION\n",
        "\n",
        "try:\n",
        "  bucket = client.get_bucket(BUCKET_TO_SAVE)\n",
        "  print(f\"Bucket {BUCKET_TO_SAVE} exists.\")\n",
        "except:\n",
        "  print(f\"Creating bucket {BUCKET_TO_SAVE}\")\n",
        "  bucket.create()\n",
        "  print(f\"Bucket {BUCKET_TO_SAVE} created.\")\n",
        "\n",
        "\n",
        "# Check if the wikipedia_strings.csv file exists locally\n",
        "if os.path.exists(\"wikipedia_strings.csv\"):\n",
        "  # Copy the file\n",
        "  #client.copy_file('wikipedia_strings.csv', f'gs://{BUCKET_TO_SAVE}/wikipedia_strings.csv')\n",
        "  blob = bucket.blob(\"wikipedia_strings.csv\")\n",
        "  blob.upload_from_filename(\"wikipedia_strings.csv\")\n",
        "  print(f\"Blob {BUCKET_TO_SAVE}/wikipedia_strings.csv uploaded to {BUCKET_TO_SAVE}.\")\n",
        "else:\n",
        "  print(\"The wikipedia_strings.csv file does not exist.\")\n",
        "  print (\"Please save the dataframe in the above frames\")\n",
        "\n",
        "stats = storage.Blob(bucket=bucket, name=\"wikipedia_strings.csv\").exists(client)\n",
        "if stats:\n",
        "  print(f\"Blob gs://{BUCKET_TO_SAVE}/wikipedia_strings.csv exists.\")\n",
        "else:\n",
        "  print(f\"Blob gs://{BUCKET_TO_SAVE}/wikipedia_strings.csv does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMGi_hiWswXq",
        "outputId": "0da1f021-3158-4aa7-b38c-2c2846c579e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-26-c0f4fd1a3b42>:7: DeprecationWarning: Assignment to 'Bucket.location' is deprecated, as it is only valid before the bucket is created. Instead, pass the location to `Bucket.create`.\n",
            "  bucket.location = REGION\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bucket wikipedia_strings_demogct2022 exists.\n",
            "Blob wikipedia_strings_demogct2022/wikipedia_strings.csv uploaded to wikipedia_strings_demogct2022.\n",
            "Blob wikipedia_strings_demogct2022/wikipedia_strings.csv exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Completed\n",
        "\n",
        "Now you have completed crawling and creating the data.\n",
        "In the next section we will load this data into matching Engine"
      ],
      "metadata": {
        "id": "a16ZO0na17rw"
      }
    }
  ]
}